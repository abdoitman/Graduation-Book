\chapter{Channel Coding}

%--------- Intro ---------%
\section{Overview}
This part deals with linear block codes covering their fundamental concepts, generator and parity check matrices, error-correcting capabilities, encoding and decoding, and performance analysis. The linear block code discussed in this part is Hamming code.

\subsection{Shannon's Noisy Channel Coding Theorem}
Any channel affected by noise possesses a specific “channel capacity” C a rate of conveying information that can never be exceeded without error, but in principle, an error-correcting code always exists such that information can be transmitted at rates less than C with an arbitrarily low BER.

\subsection{Channel Coding Principle}
The channel coding principle is to add redundancy to minimize error rate as illustrated in Figure~\ref{fig:channel coding principle}.
\begin{figure}[ht]
    \centering
    \includegraphics[width= 0.8\textwidth]{au-logo-bl.PNG}
    \caption{Illustration of the channel coding principle.}
    \label{fig:channel coding principle}
\end{figure}

\subsection{Channel Coding Gain}
The bit error rate (BER) is the probability that a binary digit transmitted from the source received erroneously by the user. For required BER, the difference between the powers required for without and with coding is called the coding gain. A typical plot of BER versus Eb/No (bit energy to noise spectral density ratio) with and without channel coding is shown in Figure 2.2. It can be seen that coding can arrive at the same value of the BER at lower Eb/No than without coding. Thus, the channel coding yields coding gain which is usually measured in dB. Also, the coding gain usually increases with a decrease in BER.
\begin{figure}[ht]
    \centering
    \includegraphics[width= 0.8\textwidth]{au-logo-bl.PNG}
    \caption{Coding gain.}
\end{figure}
%-------------------------%

%--------- Linear --------%
\section{Block Codes}
The data stream is broken into blocks of $k$ bits and each k-bit block is encoded into a block of n bits with $n > k$ bits as illustrated in Figure 2.3. The n-bit block of the channel block encoder is called the code word. The code word is formed by adding $(n - k)$ parity check bits derived from the $k$ message bits.
\begin{figure}[ht]
    \centering
    \includegraphics[width= 0.8\textwidth]{au-logo-bl.PNG}
    \caption{Coded data stream.}
\end{figure}

\subsection*{Properties of Block Codes}

\subsubsection*{Block code rate}
The block code rate (R) is defined as the ratio of $k$ message bits and length of the code word $n$.
\[R = \frac{k}{n} \]

\subsubsection*{Code word weight}
The weight of a code word or error pattern is the number of nonzero bits in the code word or error pattern. \\
For example, the weight of a code word $c = (1, 0, 0, 1, 1, 0, 1, 0)$ is 4.

\subsubsection*{Hamming distance}
The Hamming distance between two blocks v and w is the number of coordinates in which the two blocks differ.
\[ d_{hamming}(v,w) = d(v,w) = | \{i | v_i \neq w_i, i = 0,1,\ldots,n-1 \} | \]
Example: Consider the code words $v$ = (00100) and $w$ = (10010), then the Hamming distance $d_{hamming}(v, w) = 3$. \\
Hamming distance allows for a useful characterization of the error detection and error correction capabilities of a block code as a function of the code's minimum distance.

\subsubsection*{The Minimum Distance of a Block Code}
The minimum distance of a block code $C$ is the minimum Hamming distance between all distinct pairs of code words in $C$.
\begin{GrayBox}
    \textbf{A code with minimum distance $d_{min}$ can:}
    \begin{itemize}
        \item \emph{detect} all error patterns of weight less than or equal to $(d_{min} - 1)$.
        \item \emph{correct} all error patterns of weight less than or equal to $(d_{min} - 1) / 2$.
    \end{itemize}
\end{GrayBox}
\textbf{Example:} Consider the binary code $C$ composed of the following four code words.
\[ C = \{(00100), (10010), (01001), (11111)\} \]
Hamming distance of (00100) and (10010) = 3 \\
Hamming distance of (10010) and (01001) = 4 \\
Hamming distance of (00100) and (01001) = 3 \\
Hamming distance of (10010) and (11111) = 3 \\
Hamming distance of (00100) and (11111) = 4 \\
Hamming distance of (01001) and (11111) = 3 \\
Therefore, the minimum distance $d_{min}$ = 3.

\subsection{Linear Block Codes}
A block code $C$ consisting of n-tuples $\{(c_0, c_1, . . . , c_{n-1})\}$ of symbols from $GF(2)$ is said to be binary linear block code if and only if $C$ forms a vector subspace over $GF(2)$. 
\begin{description}
    \item[Note:] finite fields are also called Galois fields (GF).
\end{description}
The code word is said to be systematic linear code word if each of the $2^k$ code words is represented as linear combination of $k$ linearly independent code words.

\subsubsection{Linear Block Codes Properties}
There are \emph{two important properties} of linear block codes which are:
\begin{description}
    \item[Property 1:] The linear combination of any set of code words is a code word.
    \item[Property 2:] The minimum distance of a linear block code is equal to the minimum weight of any nonzero word in the code.
\end{description}
Also, there are 2 well-known bounds on the minimum distance which are
\begin{description}
    \item[Singleton Bound:]
    The minimum distance of an $(n, k)$ linear code is bounded by
    \begin{equation}
        \label{eq:Singleton Bound}
        d_{min} \leq n-k+1
    \end{equation}
    \item[Hamming Bound:]
    An $(n, k)$ block code can \emph{detect} $t_{ed}$ errors per code word and can correct up to $t_{ec}$ errors per code word, provided that $n$ and $k$ satisfy the Hamming bound.
    \begin{equation}
        \label{eq:Hamming Bound}
        2^{n-k} \geq \sum_{i = 0}^{t_{ec}} \binom{n}{i} \quad \text{where} \quad \binom{n}{i} = \frac{n!}{(n-1)!i!}
    \end{equation}
    \[ t_{ec} = \frac{(d_{min} - 1)}{2} \quad , \quad t_{ed} = d_{min} - 1  \]
    The relation is the upper bound on $d_{min}$ and is known as \emph{the Hamming bound}.
\end{description}
 
\subsection{Generator \& Parity Check Matrices}
Let $\{g_0, g_1, \ldots  , g_{k-1}\}$ be a basis of code words for the $(n, k)$ linear block code $C$ and $m = [m_0, m_1, \ldots , m_{k-1}]$ the message to be encoded. The Theorem that says the code word $c = (c_0, c_1, \ldots , c_{n-1})$ for the message is uniquely represented by the following linear combination of $g_0, g_1, \ldots , g_{k-1}$
\[ c = m_0g_0 + \ldots + m_{k-1}g_{k-1} \]

for every code word $c \in C$.\\
Since every linear combination of the basis elements must also be a code word, there is a one-to-one mapping between the set of k-bit blocks $(a_0, a_1, \ldots , a_{k-1})$ over $GF(2)$ and the code words in $C$. A matrix $G$ is constructed by taking the vectors in the basis as its rows.
\[ G = \left[ \begin{array}{c}
    g_0 \\
    g_1 \\
    \vdots \\
    g_{k-1}
\end{array} \right] = \left[ \begin{array}{c c c c}
    g_{0,0} & g_{0,1} & \ldots & g_{0,n-1} \\
    g_{1,0} & g_{1,1} & \ldots & g_{1,n-1} \\
    \vdots & \vdots & \ddots & \vdots \\
    g_{k-1,0} & g_{k-1,1} & \ldots & g_{k-1,n-1}
\end{array} \right]
\]
This matrix $G$ is a generator matrix for the code $C$. It can be used to directly encode k-bit blocks in the following manner:
\[
mG = [m_0, m_1, \ldots, m_{k-1}] \cdot \left[ \begin{array}{c}
    g_0 \\
    g_1 \\
    \vdots \\
    g_{k-1}
\end{array} \right] = m_0g_0 + m_1g_1 + \ldots + m_{k-1}g_{k-1} = c
\]

The dual space of a linear block code $C$ is the dual code of $C$ and a basis $\{h_0, h_1, \ldots , h_{n-k-1}\}$ can be found for dual code of $C$, and the following parity check matrix can be constructed:
\[
H = \left[ \begin{array}{c}
    h_0 \\
    h_1 \\
    \vdots \\
    h_{n-k-1}
\end{array} \right] = \left[ \begin{array}{c c c c}
    h_{0,0} & h_{0,1} & \ldots & h_{0,n-1} \\
    h_{1,0} & h_{1,1} & \ldots & h_{1,n-1} \\
    \vdots & \vdots & \ddots & \vdots \\
    h_{n-k-1,0} & g_{n-k-1,1} & \ldots & g_{n-k-1,n-1}
\end{array} \right]
\]

In a systematic linear block code, the last $k$ bits of the codeword are \emph{the message bits}, that is
\[ c_i = m_{i-(n-k)} \quad , \quad i = n-k, \ldots , n-1 \]
While the first $n-k$ bits in the codeword are check bits generated from the k message bits according to
\[ c_0 = p_{0,0} m_0 + p_{1,0} m_1 + \ldots + p_{k-1, 0} m_{k-1} \]
\[ c_1 = p_{0,1} m_0 + p_{1,1} m_1 + \ldots + p_{k-1, 1} m_{k-1} \]
\[ \vdots \]
\[ c_{n-k-1} = p_{0,n-k-1} m_0 + p_{1,n-k-1} m_1 + \ldots + p_{k-1, n-k-1} m_{k-1} \]

The above equation can be written in a matrix form as:
\begin{equation}
    \label{equation:c=mG}
    [ c_0, c_1, \ldots, c_{n-1} ] = [ m_0, m_1, \ldots, m_{k-1} ] \left[ \begin{array}{c c c c c c c}
        p_{0,0} & p_{0,1} & \cdots & p_{0,n-k-1} & 1000 & \cdots & 0 \\
        p_{1,0} & p_{1,1} & \cdots & p_{1,n-k-1} & 0100 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
        p_{k-1,0} & p_{k-1,1} & \cdots & p_{k-1,n-k-1} & 0000 & \cdots & 1 \\     
    \end{array} \right]_{k \times n}
\end{equation}
\[ \text{or} \]
\[ c = mG \]
where $G$ is the matrix on the right hand side of the equation~\ref{equation:c=mG}. \\
The $k \times n$ matrix $G$ is called a generator matrix of the code and it has the following form:
\begin{equation}
    \label{eq:G}
    G = \left[ P | I_k \right]_{k \times n}
\end{equation}

The matrix $I_k$ is the identity matrix of order $k$, and $P$ is an arbitrary $k \times n-k $ matrix. When $P$ is specified, it defines the $(n, k)$ block code completely. The parity check matrix H corresponding to the above generator matrix G can be obtained as
\[ H = \left[ I_{n-k} | P^T \right] \]
\begin{equation}
    \label{equation:H}
    H = \left[ \begin{array}{c c c c c c c}
        1000 & \cdots & 0 & p_{0,0} & p_{0,1} & \cdots & p_{0,n-k-1} \\
        0100 & \cdots & 0 & p_{1,0} & p_{1,1} & \cdots & p_{1,n-k-1} \\
        \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0000 & \cdots & 1 & p_{0,n-k-1} & p_{1,n-k-1} & \cdots & p_{k-1,n-k-1} \\     
    \end{array} \right]
\end{equation}

\newtcbtheorem[auto counter,number within=section]{theo}
  {Theorem}{fonttitle=\bfseries\upshape, fontupper=\slshape,
     arc=0mm, colback=black!5!white,colframe=black!75!white}{theorem}

\begin{theo}{Parity Check Theorem}{}
    For any $(n, k)$ linear block code $C$ with $(n-k)\times n$ parity check matrix $H$, a code word $c \in C$ is a valid code word \textbf{if and only if} $cH^{T}=0$.
    \par\noindent\dotfill \\
    \textbf{Example:}\\
    For the following generator matrix of (7,4) block code. Find the code vector for the message vector m = (1110) and check the validity of code vector generated.
    \[
    G = \left[ \begin{array}{c c c | c c c c}
        1 & 1 & 0 & 1 & 0 & 0 & 0 \\
        0 & 1 & 1 & 0 & 1 & 0 & 0 \\
        1 & 1 & 1 & 0 & 0 & 1 & 0 \\
        1 & 0 & 1 & 0 & 0 & 0 & 1 
    \end{array} \right]
    \]
    \textbf{Solution:}\\
    The code vector for the message block $m = (1110)$ is given by
    \[
    c = mG = [1 1 1 0] \cdot \left[ \begin{array}{c c c | c c c c}
        1 & 1 & 0 & 1 & 0 & 0 & 0 \\
        0 & 1 & 1 & 0 & 1 & 0 & 0 \\
        1 & 1 & 1 & 0 & 0 & 1 & 0 \\
        1 & 0 & 1 & 0 & 0 & 0 & 1 
    \end{array} \right] = [0 1 0 1 1 1 0]
    \]
    \[
    H = \left[ \begin{array}{c c c | c c c c}
        1 & 0 & 0 & 1 & 0 & 1 & 1 \\
        0 & 1 & 0 & 1 & 1 & 1 & 0 \\
        0 & 0 & 1 & 0 & 1 & 1 & 1 
    \end{array} \right]
    \]
    \[
    cH^T = [0 1 0 1 1 1 0] \cdot \left[ \begin{array}{c c c}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1 \\
        1 & 1 & 0 \\
        0 & 1 & 1 \\
        1 & 1 & 1 \\
        1 & 0 & 1 
    \end{array} \right] = [0 0 0]
    \]
    Hence, the generated code vector is valid.
\end{theo}

\section{Hamming Codes}
Hamming code is a linear block code capable of correcting single errors having a minimum distance $d_{min} = 3$. It is very easy to construct Hamming codes. The parity check matrix $H$ (obtained in equation~\ref{equation:H}) must be chosen so that no row in $H^T$ is zero and the first $(n-k)$ rows of $H^T$ form an identity matrix and all the rows are distinct. We can select $2^{n-k}-1$ distinct rows of $H^T$.\\
Since the matrix $H^T$ has $n$ rows, for all of them to be distinct, the following inequality should be satisfied.
\[ 2^{n-k} -1 \geq n \]
Implying that
\[ (n-k) \geq \log_2(n+1) \]
\begin{equation}
    \label{eq:n geq klog}
    n \geq k + \log_2(n+1)
\end{equation}
Hence, the minimum size $n$ for the code words can be determined from equation~\ref{eq:n geq klog}. \\

\textbf{Example:} Design a Hamming code with message block size of eleven bits. \\
\textbf{Solution:} It follows from equation~\ref{eq:n geq klog} that
\[ n \geq 11 + \log_2(n+1) \]
The smallest $n$ that satisfies the above inequality is 15, and hence, we need a $(15,11)$ block code. Thus, the transpose of the parity check matrix $H$ will be $4 \times 15$ matrix. The first four rows of $H^T$ will be $I_4$ matrix. The last eleven rows are arbitrarily chosen, with the restrictions that no row is zero, and all the rows are distinct.
\begin{equation}
    \label{eq:H solution}
    H^T = \left[ \begin{array}{c c c c}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1 \\
        \hline
        0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 \\
        0 & 1 & 1 & 1 \\
        0 & 0 & 1 & 1 \\
        1 & 0 & 0 & 1 \\
        1 & 0 & 1 & 0 \\
        1 & 0 & 1 & 1 \\
        1 & 1 & 0 & 0 \\
        1 & 1 & 0 & 1 \\
        1 & 1 & 1 & 0 \\
        1 & 1 & 1 & 1
    \end{array} \right] = \left[ \begin{array}{c}
        I_{n-k} \\
        \hline
        P^T    
    \end{array} \right]
\end{equation}

Then generator matrix $G$ equals
\[
G = \left[ \begin{array}{c c c c c c c c c c c c c c c}
    0&1&0&1&1&0&0&0&0&0&0&0&0&0&0 \\
    0&1&1&0&0&1&0&0&0&0&0&0&0&0&0 \\
    0&1&1&1&0&0&1&0&0&0&0&0&0&0&0 \\
    0&0&1&1&0&0&0&1&0&0&0&0&0&0&0 \\
    1&0&0&1&0&0&0&0&1&0&0&0&0&0&0 \\
    1&0&1&0&0&0&0&0&0&1&0&0&0&0&0 \\ 
    1&0&1&1&0&0&0&0&0&0&1&0&0&0&0 \\
    1&1&0&0&0&0&0&0&0&0&0&1&0&0&0 \\
    1&1&0&1&0&0&0&0&0&0&0&0&1&0&0 \\
    1&1&1&0&0&0&0&0&0&0&0&0&0&1&0 \\
    1&1&1&1&0&0&0&0&0&0&0&0&0&0&1
\end{array} \right]
\]

\textbf{Example:} Construct parity check and generator matrices for a $(7, 4)$ Hamming code. \\
\textbf{Solution:} The parity check matrix $H$ and generator matrix $G$ for a $(7, 4)$ Hamming code are
\[
H = \left[ \begin{array}{c c c c c c c}
    1 & 0 & 0 & 1 & 0 & 1 & 1 \\
    0 & 1 & 0 & 1 & 1 & 1 & 0 \\
    0 & 0 & 1 & 0 & 1 & 1 & 1
\end{array} \right]
\]
\[
G = \left[ \begin{array}{c c c c c c c}
    1 & 1 & 0 & 1 & 0 & 0 & 0 \\
    0 & 1 & 1 & 0 & 1 & 0 & 0 \\
    1 & 1 & 1 & 0 & 0 & 1 & 0 \\
    1 & 0 & 1 & 0 & 0 & 0 & 1
\end{array} \right]
\]

\subsection{Syndrome Table Decoding}
Consider a valid code word $c$ for transmission and let $e$ be an error pattern introduced by the channel during transmission. Then, the received vector $r$ can be written as
\[ r = c + e \]
Multiplying the $r$ by the transpose of the parity check matrix gives the syndrome $S$ which can be expressed as \\
\begin{equation}
    \label{eq:Syndrome Table Decoding}
    \begin{aligned}
        S &= r \cdot H^T \\
        &= (c+e) \cdot H^T \\
        &= cH^T + eH^T\\
        &= 0 + eH^T \\ 
        &= eH^T
    \end{aligned}
\end{equation}

Thus, the syndrome vector is independent of the transmitted code word $c$ and is only a function of the error pattern $e$. Decoding is performed by computing the syndrome of a received vector, looking up the corresponding error pattern, and subtracting the error pattern from the received word. \\

\textbf{Example:} Construct a syndrome decoding table for a $(7, 4)$ Hamming code \\
\textbf{Solution:} For a $(7, 4)$ Hamming code, there are $2^{(7-4)}$ error patterns $e$ as in Table~\ref{eq:Syndrome Table Decoding}
\begin{table}[!ht]
    \centering
    \caption{Syndrome decoding table for a $(7, 4)$ Hamming code}
    \label{tbl:syndrome decoding table}
    \begin{tabular}{cc}
        \toprule
        Error Pattern $e$ & Syndrome \\
        \midrule
        0000000 & 000 \\
        1000000 & 100 \\
        0100000 & 010 \\
        0010000 & 001 \\
        0001000 & 110 \\
        0000100 & 011 \\
        0000010 & 111 \\
        0000001 & 101 \\
        \bottomrule
    \end{tabular}
\end{table}

The syndrome for $(7, 4)$ Hamming code is computed using the parity check matrix $H$ (as given in the solution~\ref{eq:H solution}) as follows
\[ S = e \cdot H^T \]
Thus, the syndrome decoding table for a $(7, 4)$ Hamming code is as in Table~\ref{tbl:syndrome decoding table}

\subsection{Hamming Codes Decoding}
Syndrome table is used to decode the Hamming codes. The syndrome table gives the syndrome value based on the simple relationship with parity check matrix. The single-error-correcting codes (i.e., Hamming codes), are decoded by using syndrome value. Consider a code word $c$ corrupted by $e$, an error pattern with a single one in the $j^{th}$ coordinate position results a received vector $r$. Let ${h_0, h_1, \ldots , h_{n-1}}$ be the set of columns of the parity check matrix $H$. When the syndrome is computed, we obtain the transposition of the $j^{th}$ column of $H$. \\
\begin{equation}
    \label{eq:hamming codes decoding}
    s = eH^T = [0, \ldots, 0,1,0, \ldots, 0] \cdot \left[ \begin{array}{c}
        h_0^T \\
        h_1^T \\
        \vdots \\
        h_{n-1}^T
    \end{array} \right] = h_j^T
\end{equation}

The above-mentioned process in equation~\ref{eq:hamming codes decoding} can be implemented using the following algorithm:
\begin{enumerate}
    \item Compute the syndrome $s$ for the received word. If $s = 0$, the received code word is the correct code word.
    \item Find the position $j$ of the column of $H$ that is the transposition of the syndrome.
    \item Complement the $j^{th}$ bit in the received codeword to obtain the corrected code word. \\
\end{enumerate}

\textbf{Example:} Decode the received vector $r = [001100011100000]$ using the $(15,11)$ parity check matrix. \\
\textbf{Solution:} \\
\[
H = \left[ \begin{array}{c c c c c c c c c c c c c c c}
    1&0&0&0&0&0&0&0&1&1&1&1&1&1&1 \\
    0&1&0&0&1&1&1&0&0&0&0&1&1&1&1 \\
    0&0&1&0&0&1&1&1&0&1&1&0&0&1&1 \\
    0&0&0&1&1&0&1&1&1&0&1&0&1&0&1    
\end{array} \right]
\] \\
The received vector is $r = [001100011100000]$ \\
The corresponding syndrome $s = r \cdot H^T$ is \\
\[s = [0011]\] \\
The syndrome is the transposition of $7^{th}$ column of $H$. Inverting the $7^{th}$ coordinate of $r$, the following code word is obtained \\
\[c = [001100001100000]\]


\section{LDPC Coding}